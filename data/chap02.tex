\chapter{单一尺度下的故障诊断模型研究}
\label{cha:chapter2}

\section{引言}

本章主要研究了单一尺度的特征提取方法及故障诊断模型，特征提取过程使用了平均值下采样的方法。
由于下采样的过程中我们可以指定下采样的核宽度，因此我们能够得到不同维度的特征向量（即不同尺度
的特征）。本章也通过实验对比研究了特征的不同个数对模型最终的预测精度的影响，从而选择最佳
的特征个数。

本章首先对特征提取过程中用到的实数形式的离散傅立叶变换以及故障诊断模型中用到的支持向量机算法
的基本原理做了简单的介绍，然后详细介绍了本章中设计的单一尺度的特征提取方法，以及基于支持向量
机的级联分类模型，之后简单地介绍了本文各章的实验中都用到的数据集CWRU滚动轴承故障数据集，最后
使用该数据集对本章设计的模型进行了验证。

\section{基本理论}

\subsection{实数形式的傅立叶变换}
\label{subsection:rdft}

离散傅立叶变换（Discrete Fourier Transform, DFT)是信号处理中常用的一种技术，当信号从时域变换到
频域之后，能够将原始信号中包含的信息分为相位和幅度信息，
往往能够更好地反映出信号的频谱结构和变化规律，从而简化信号分析的难度。在信号识别或分
类等应用中，DFT经常被用来提取信号的频域特征。对于具有N个采样点的信号序列$\{x_n\}_{0\leq n < N}$，
它的傅立叶变换如~(\ref{equ:chap2:dft})式所示。
\begin{equation}
\label{equ:chap2:dft}
  \hat{x}_k=\sum_{n=0}^{N-1}x_n e^{-i\frac{2\pi}{N}nk}, k=0,1,...,N-1
\end{equation}

实数形式的离散傅立叶变换（Real Discrete Fourier transform, RDFT）是由Okan Ersoy等人于1985年
提出的一种酉变幻~\cite{ersoy1985real}。RDFT在数据压缩、滤波等很多信号处理应用中的效果比DFT更
加出色~\cite{ersoy1988fast}。同样，RDFT也有类似于快速傅立叶变换
的快速计算方法RFFT~\cite{ersoy1988fast}。同样，对于具有N个采样点的信号序列$\{x_n\}_{0\leq n < N}$，
它的实数形式的傅立叶变换如~(\ref{equ:chap2:rdft})式所示。
\begin{equation}
\label{equ:chap2:rdft}
  \widetilde{x}=
  \begin{cases}
    \left[\hat{x}_0, \text{Re}\hat{x}_1, \text{Im}\hat{x}_1, ..., \text{Re}\hat{x}_{\frac{N}{2}}\right],  & N\text{为偶数}\\
    \left[\hat{x}_0, \text{Re}\hat{x}_1, \text{Im}\hat{x}_1, ..., \text{Im}\hat{x}_{\frac{N-1}{2}}\right],& N\text{为奇数}
  \end{cases}
\end{equation}

本文在信号序列的特征提取阶段使用RDFT而不是DFT的原因如下：

1）实值性

从~(\ref{equ:chap2:dft})式和~(\ref{equ:chap2:rdft})式中可以看出，DFT的结果是一个复数序列，
它的快速计算方法FFT涉及复数加法和乘法。但是在原始信号序列为实数序列时，
RDFT的计算结果仍然是一个实数序列，因此其计算过程完全不需要用到复数的加法和乘法，从而降低了
软硬件实现的复杂性，同时也便于显示和进一步处理。

2）特征个数少

从DFT的定义~(\ref{equ:chap2:dft})式可以看出，DFT的结果具有~(\ref{equ:chap2:symmetry})式所示
的复共轭对称性。
\begin{equation}
\label{equ:chap2:symmetry}
  \hat{x}_n = \hat{x}^*_{N-n}
\end{equation}

因此，在DFT计算所得的序列中，有一半的数据是冗余的，不但增加了计算量，而且会增加随后的分类器
的复杂程度。相反，~(\ref{equ:chap2:rdft})式表示的RDFT在定义的时候就避免了这种冗余，实现起来
更加高效。因此，RDFT在处理很多实信号时，比DFT更加具有优势。

\subsection{支持向量机}

支持向量机是最常用的有监督学习算法之一。在介绍SVM算法之前，我们首先需要讨论间隔的概念
以及最大间隔分类的思想。接下来，我们会讨论最优间隔分类器以及拉格朗日对偶问题。最后一部分
将介绍使SVM算法能够有效应用于高维特征空间及非线性分类问题的重要技巧：核函数。

\subsubsection{间隔}

在逻辑斯特回归（Logistic Regression）算法中，我们用$h_{\theta}(x) = g(\theta^Tx)$对概率
$p(y = 1|x;\theta)$进行建模。对于输入$x$，当且仅当$h_{\theta}(x) \geq 0.5$
即$\theta^Tx \geq 0$时，我们预测$y = 1$。对于训练集中的一个正样本（即$y = 1$），$\theta^Tx$
越大，$h_{\theta}(x) = p(y = 1|x;\theta)$也就越大，意味着标签为1的置信度越高。
因此我们可以得出一个直观的结论，当$\theta^Tx \gg 0$时，我们预测$y = 1$的置信度是非常高的。
同样地，我们可以认为在$\theta^Tx \ll 0$时，逻辑斯特回归模型得出的$y = 0$的预测结果也是非常
可信的。给定一个训练集，直观上只要我们能够找到一个$\theta$使得对任意$y^{(i)} = 1$有
$\theta^Tx^{(i)} \gg 0$并且对任意$y^{(i)} = 0$有$\theta^Tx^{(i)} \ll 0$，那么我们将会
获得一个非常好的拟合效果。因为在这种情况下，对于所有训练样本，我们都能得到一个置信度很高
的分类结果。之后我们会用函数间隔的概念来正式表述这种思想。

下面我们考虑如图~\ref{fig:margin}所示的二分类问题，图中叉号表示训练集中的正样本，圆圈
表示训练集中的负样本，两类样本之间的直线表示决策边界（也称为分类超平面，由$\theta^Tx = 0$
给出）。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics{margin}
  \caption{二分类问题}
  \label{fig:margin}
\end{figure}

从图中可以看到A点离决策边界非常远，如果我们在A点处对$y$的值进行预测，我们可以得到一个置信度
非常高的结果$y = 1$。相反，C点离决策边界非常近，虽然我们根据它在决策边界右侧可以预测得到
$y = 1$，但是即便是决策边界的一个微小的变化都会轻易地导致预测结果变为$y = 0$。也就是说A点
处的预测结果比C点处的预测结果具有更高的置信度，而B点处于这两种情况之间。一般来讲，如果一个点
离分类超平面越远，我们的预测结果的置信度就越高。在给定一个训练集的情况下，我们希望找到一个
决策边界能够对训练集中的所有样本都能做出正确并且置信度高的预测。之后我们会用几何间隔的概念
来正式表述这种思想。

\subsubsection{符号定义}

为了表述方便，我们首先需要引入一种新的记法来讨论分类问题。重新明确一下要解决的问题：特征为
$x$，标签为$y$的二分类问题，求一个线性分类器。从现在开始，我们使用$y \in \{-1, 1\}$（而不是
$\{0, 1\}$）来表示类别标签。另外，我们用参数$w, b$（而不是$\theta$）来表示我们的线性分类器，
如~(\ref{equ:chap2:classifier})式所示。
\begin{equation}
  \label{equ:chap2:classifier}
  h_{w,b}(x) = g(w^Tx+b)
\end{equation}

此处如果$z \geq 0$那么$g(z) = 1$，否则$g(z) = -1$。

\subsubsection{函数间隔和几何间隔}

下面正式给出函数间隔和几何间隔的定义。给定一个训练样本$(x^{(i)},y^{(i)})$，定义$(w,b)$相对
于训练样本的函数间隔如~(\ref{equ:chap2:fmargin_i})式所示。
\begin{equation}
  \label{equ:chap2:fmargin_i}
  \hat{\gamma}^{(i)} = y^{(i)}(w^Tx+b)
\end{equation}

从~(\ref{equ:chap2:fmargin_i})式可以看出，在$y^{(i)} = 1$时，为了使函数间隔尽量大（即预测结果
置信度尽量高），我们需要使$w^Tx+b$为一个尽可能大的正数。相反，在$y^{(i)} = -1$时，为了
使函数间隔尽量大，$w^Tx+b$需要为一个尽可能小的负数。此外，$y^{(i)}(w^Tx+b) > 0$意味着我们的
预测结果是正确的。因此，大的函数间隔代表了准确且高置信度的预测结果。

给定一个训练集$S = \{(x^{(i)},y^{(i)});i = 1, ..., m\}$，定义$(w,b)$关于$S$的函数边界为
所有训练样本的函数边界的最小值，记为$\hat{\gamma}$，可以用~(\ref{equ:chap2:fmargin})式表示。
\begin{equation}
  \label{equ:chap2:fmargin}
  \hat{\gamma} = \min_{i=1,...,m}\hat{\gamma}^{(i)}
\end{equation}

对于这样一个线性分类器，如果用$2w$和$2b$分别替换$w$和$b$，那么$g(w^Tx+b)=g(2w^Tx+2b)$，$h_{w,b}(x)$
将保持不变。也就是说$g$和$h_{w,b}(x)$仅仅取决于$w^Tx+b$的符号而不是大小。然而将$(w,b)$替换为
$(2w,2b)$会导致函数间隔变为原来的两倍。因此，我们需要引入一些归一化条件，比如令$\|w\|=1$。

下面，我们讨论几何边界。考虑图~\ref{fig:margin2}所示的分类问题：
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics{margin2}
  \caption{几何间隔}
  \label{fig:margin2}
\end{figure}

图中画出了与$(w,b)$所对应的决策边界，以及向量$w$。显然，$w$是垂直于分类超平面的。A点是某个
训练样本的输入$x^{(i)}$，其标签为$y^{(i)} = 1$，到决策边界的距离$\gamma^{(i)}$由线段AB给出。

下面来计算$\gamma^{(i)}$的值。由于$w/\|w\|$是一个与$w$同方向的单位向量，并且A点坐标为$x^{(i)}$，
因此我们可以得到B点的坐标为$x^{(i)} - \gamma^{(i)} \cdot w/\|w\|$。进一步由于B点在决策边界上，
而所有决策边界上的点$x$均满足$w^Tx+b=0$，因此得到：
\begin{equation}
  \label{equ:chap2:gmargin_i_1}
  w^T\left(x^{(i)} - \gamma^{(i)}\frac{w}{\|w\|}\right) + b = 0
\end{equation}

解得：
\begin{equation}
  \label{equ:chap2:gmargin_i_2}
  \gamma^{(i)} = \frac{w^Tx^{(i)}+b}{\|w\|} = \left(\frac{w}{\|w\|}\right)^Tx^{(i)} + \frac{b}{\|w\|}
\end{equation}

~(\ref{equ:chap2:gmargin_i_2})式对任意正样本（即$y^{(i)} = 1$）成立。对于一般情况，
定义$(w,b)$关于训练样本$(x^{(i)}, y^{(i)})$的几何间隔为：
\begin{equation}
  \label{equ:chap2:gmargin_i_3}
  \gamma^{(i)} = y^{(i)}\left(\left(\frac{w}{\|w\|}\right)^Tx^{(i)} + \frac{b}{\|w\|}\right)
\end{equation}

从~(\ref{equ:chap2:gmargin_i_3})式可以看出，如果$\|w\|=1$，那么函数间隔将等于几何间隔。可以看出
与函数间隔不同的是如果我们用$2w$和$2b$分别替换$w$和$b$，那么几何间隔将不会发生改变。也正是由于
几何间隔相对于参数缩放的这种不变性，当我们使用训练数据来拟合$w$和$b$的时候，我们可以引入任意
的缩放约束，而不会改变模型本身。比如，可以令$\|w\|=1$，或者$|w_1|=5$，或者$|w_1+b|+|w_2|=2$等。

同样，给定一个训练集$S=\{(x^{(i)},y^{(i)});i=1,...,m\}$，定义$(w,b)$关于$S$的几何间隔
为所有训练样本的几何间隔的最小值，如~(\ref{equ:chap2:gmargin})式所示。
\begin{equation}
  \label{equ:chap2:gmargin}
  \gamma=\min_{i=1,...,m}\gamma^{(i)}
\end{equation}

\subsubsection{最优间隔分类器}

给定一个训练集，从前面的讨论可以得到一个自然的结论：通过寻找最大化几何间隔的决策边界，就能得到
一个在训练集上拟合得很好的分类器。假设训练集上的两类样本是线性可分的，即存在分类超平面可以完全
将两类样本分开，那么寻找最大化几何间隔的超平面的问题可以表述为~(\ref{equ:chap2:opt1})式所示的优化
问题。
\begin{equation}
  \label{equ:chap2:opt1}
  \begin{aligned}
    \max_{\gamma,w,b} &
    & & \gamma \\
    \text{s.t.} &
    & & y^{(i)}\left(w^Tx^{(i)}+b\right)\geq \gamma, i=1,...,m \\
    \quad &
    & & \|w\|=1
  \end{aligned}
\end{equation}

即最大化$\gamma$，使得每个训练样本的函数间隔都至少是$\gamma$。约束条件$\|w\|=1$保证了函数间隔和
几何间隔相等，从而保证了所有训练样本的几何间隔也至少是$\gamma$。

由于约束条件$\|w\|=1$是非凸的，因此这个问题不易直接用标准优化软件工具求解。因此将其转化为
~(\ref{equ:chap2:opt2})式所示的优化问题。
\begin{equation}
  \label{equ:chap2:opt2}
  \begin{aligned}
    \max_{\hat{\gamma},w,b} &
    & & \frac{\hat{\gamma}}{\|w\|} \\
    \text{s.t.} &
    & & y^{(i)}\left(w^Tx^{(i)}+b\right)\geq \hat{\gamma}, i=1,...,m
  \end{aligned}
\end{equation}

由于几何间隔和函数间隔之间存在关系$\gamma=\hat{\gamma}/\|w\|$，因此我们最大化$\hat{\gamma}/\|w\|$，
使得所有训练样本的函数间隔至少为$\hat{\gamma}$。这样虽然去掉了约束条件$\|w\|=1$，但是引入了非凸
的目标函数$\frac{\hat{\gamma}}{\|w\|}$。

前面提到我们可以在$w$和$b$上增加任意一个缩放约束条件而不改变问题本身。因此我们可以令$w,b$关于训练集
的函数间隔等于1。
\begin{equation}
  \label{equ:chap2:constrain}
  \hat{\gamma} = 1
\end{equation}

将~(\ref{equ:chap2:constrain})式代入~(\ref{equ:chap2:opt2})，并将最大化$\hat{\gamma}/\|w\|=1/\|w\|$
替换为最小化$\frac{1}{2}\|w\|^2$，可以得到~(\ref{equ:chap2:opt3})式所示的优化问题。
\begin{equation}
  \label{equ:chap2:opt3}
  \begin{aligned}
    \min_{w,b} &
    & & \frac{1}{2}\|w\|^2 \\
    \text{s.t.} &
    & & y^{(i)}\left(w^Tx^{(i)}+b\right)\geq 1, i=1,...,m
  \end{aligned}
\end{equation}

\subsubsection{拉格朗日对偶问题}

虽然~(\ref{equ:chap2:opt3})式表示的优化问题可以使用商业的二次规划（Quadratic Programming, QP）
代码来求解，但是一般情况下我们都是选择将其转化为拉格朗日对偶形式来求解。这对我们使用核函数来
使模型更好地应用于高维特征空间是至关重要的。另外，其对偶形式可以通过一些迭代算法来求解，
通常情况下比直接使用QP软件求解更加高效。

~(\ref{equ:chap2:opt3})式表示的原问题对应的拉格朗日函数如~(\ref{equ:chap2:lagrangian})式所示。
\begin{equation}
  \label{equ:chap2:lagrangian}
  \mathcal{L}(w,b,\alpha)=\frac{1}{2}\|w\|^2-\sum_{i=1}^m \alpha_i\left[y^{(i)}\left(w^Tx^{(i)}+b\right)-1\right]
\end{equation}

固定$\alpha$，最小化$\mathcal{L}(w,b,\alpha)$，即$\mathcal{L}$关于$w$和$b$的偏导数为0:
\begin{equation}
  \label{equ:chap2:derivative_w}
  \nabla_w\mathcal{L}(w,b,\alpha)=w-\sum_{i=1}^{m}\alpha_iy^{(i)}x^{(i)}=0
\end{equation}

解得：
\begin{equation}
  \label{equ:chap2:expression_w}
  w=\sum_{i=1}^{m}\alpha_iy^{(i)}x^{(i)}
\end{equation}

$\mathcal{L}$关于$b$的偏导数为0，得到：
\begin{equation}
  \label{equ:chap2:derivative_b}
  \frac{\partial}{\partial b}\mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_iy^{(i)}=0
\end{equation}

将~(\ref{equ:chap2:expression_w})式和~(\ref{equ:chap2:derivative_b})式代入~(\ref{equ:chap2:lagrangian})式
化简可得：
\begin{equation}
  \label{equ:chap2:dual_objective}
  \mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^m
  y^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}
\end{equation}

加上拉格朗日乘子$\alpha_i$非负的约束条件，以及~(\ref{equ:chap2:derivative_b})式的约束条件，可以得到
对偶优化问题如~(\ref{equ:chap2:dual_opt})式所示。
\begin{equation}
  \label{equ:chap2:dual_opt}
  \begin{aligned}
    \max_{\alpha} &
    & & \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^m
    y^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)},x^{(j)}\rangle\\
    \text{s.t.} &
    & & \alpha_i\geq 0, i=1,...,m \\
    \quad &
    & & \sum_{i=1}^m\alpha_iy^{(i)}=0
  \end{aligned}
\end{equation}

针对这个优化问题，已经有很多基于迭代的算法被设计出来，其中最经典的是序贯最小优化算法
（SMO, Sequential Minimal Optimization）~\cite{platt1998sequential}，
很多其他算法都是基于该算法设计的。这些算法使得我们不必直接通过二次规划的方式求解该优化问题，
保证了模型在特征维度很大或者样本数量很多的情况下仍然能够有效求解。

当我们获得对偶问题的最优解$\alpha^*$之后，可以通过~(\ref{equ:chap2:expression_w})式来计算$w^*$，
并通过~(\ref{equ:chap2:expression_b})式来计算$b^*$。
\begin{equation}
  \label{equ:chap2:expression_b}
  b^*=-\frac{\max_{i:y^{(i)}=-1}{w^*}^Tx^{(i)}+\min_{i:y^{(i)}=1}{w^*}^Tx^{(i)}}{2}
\end{equation}

最后，我们可以通过计算${w^*}^Tx+b^*$的值来对样本$x$的标签进行预测，如果该值大于0，那么将预测$y=1$,
反之则预测$y=-1$。

\subsubsection{不可分的情况及正则化}

我们在前面推导SVM模型的时候都假定数据是线性可分的，但是并不能保证事实情况总是如此。
此外，即便是能够找到这样一个分类超平面， 也不一定是就是我们想要的结果，
因为这样的分类超平面可能是受到了一些异常点的干扰。举个例子，
图~\ref{fig:margin3}显示了一个最优分类超平面，当一个异常点被加入到左上角区域时
（如~\ref{fig:margin4}所示），分类超平面产生一个非常大的变动，导致分类器的间隔急剧减小。
\begin{figure}[ht]
  \centering%
  \subcaptionbox{正常情况下的决策面\label{fig:margin3}} %标题的长度，超过则会换行，如下一个小图。
  {\includegraphics{margin3}}%
  \hspace{6em}%
  \subcaptionbox{有异常点时的决策面\label{fig:margin4}}
  {\includegraphics{margin4}}
  \caption{决策面受异常点干扰示例}
  \label{fig:margin3+4}
\end{figure}

为了使得模型在非线性可分的情况下仍然有效，同时对异常点的干扰更加鲁棒，我们将原来的优化问题
重新写为~(\ref{equ:chap2:opt_reg})式：
\begin{equation}
  \label{equ:chap2:opt_reg}
  \begin{aligned}
    \min{w,b} &
    & & \frac{1}{2}\|w\|^2+C\sum_{i=1}^m\xi_i \\
    \text{s.t.} &
    & & y^{(i)}\left(w^Tx^{(i)}+b\right)\geq 1-\xi_i, i=1,...,m \\
    \quad &
    & & \xi_i \geq 0, i=1,...,m
  \end{aligned}
\end{equation}

即允许样本的函数间隔小于1（等价于被错分），并且在目标函数中引入一个惩罚项$C\xi_i$。参数C
控制惩罚项所占的权重，被称为惩罚因子。同样地，我们可以得到该优化问题的拉格朗日对偶问题，如
~(\ref{equ:chap2:dual_opt_reg})所示。
\begin{equation}
  \label{equ:chap2:dual_opt_reg}
  \begin{aligned}
    \max_{\alpha} &
    & & \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^m
    y^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)},x^{(j)}\rangle\\
    \text{s.t.} &
    & & 0\leq\alpha_i\leq C, i=1,...,m \\
    \quad &
    & & \sum_{i=1}^m\alpha_iy^{(i)}=0
  \end{aligned}
\end{equation}

\subsubsection{核函数}

从~(\ref{equ:chap2:dual_opt_reg})中可以看出，模型可以写成一个完全关于内积$\langle x,z\rangle$的形式，
意味着如果我们使用一种特征映射$\phi$将输入特征$x$变换到空间$\phi(x)$，那么我们可以将所有的内积
替换为$\langle\phi(x),\phi(z)\rangle$。一般地，给定一种特征映射$\phi$，我们定义与之对应的核函数如
~(\ref{equ:chap2:kernel})所示。
\begin{equation}
  \label{equ:chap2:kernel}
  K(x,z)=\phi(x)^T\phi(z)
\end{equation}

特别地，当$\phi(x)=x$时，$K(x,z)=\langle x,z\rangle$，我们称之为线性核，此时模型即为
~(\ref{equ:chap2:dual_opt_reg})式所示。除此之外，还有很多常用的核函数，举例如下：

1）多项式核函数
\begin{equation}
  \label{equ:chap2:polynomial_kernel}
  K(x,z)=(x^Tz+c)^d
\end{equation}

2）高斯核函数
\begin{equation}
  \label{equ:chap2:gauss_kernel}
  K(x,z)=\exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right)
\end{equation}

3）sigmoid核函数
\begin{equation}
  \label{equ:chap2:sigmoid_kernel}
  K(x,z)=\tanh(\alpha x^Tz+c)
\end{equation}

\section{基于支持向量机的级联故障诊断模型}

\subsection{基于下采样的单一尺度特征提取方法}

给定一个信号片段$x$，其采样点个数为$N$，即$x=(x_1,x_2,...,x_N)^T$。首先我们
对$x$进行~(\ref{equ:chap2:rdft})式所示的实数形式的离散傅立叶变换，得到RDFT
频谱序列$\widetilde{x}$，由~\ref{subsection:rdft}节中的介绍可知，$\widetilde{x}$
的维度也是$N$，即$\widetilde{x} = (\widetilde{x}_1, \widetilde{x}_2, ...,
\widetilde{x}_N)^T$。为了避免样本间相位不同对最终模型的诊断精度产生影响，同
时也为了避免在后续的操作中正负值之间相互抵消，我们对RDFT频谱序列取了绝对值，
即$X = (|\widetilde{x}_1|, |\widetilde{x}_2|, ..., |\widetilde{x}_N|)^T$，本
文中之后提到的频谱序列均是指$X$。接下来我们使用核宽度为$F$的平均值下采样来进
一步提取特征，假设我们希望得到的特征个数为$L$，那么$F$和$L$满足~(\ref{equ:chap2:f_l})
式所示的关系。
\begin{equation}
  \label{equ:chap2:f_l}
  F=\lfloor N/L \rfloor
\end{equation}

其中$\lfloor x \rfloor$表示不大于$x$的最大整数。将序列$X$等分为$L$个连续不重
叠的小区间，并且对各个区间内的值进行平均，得到长度为L的特征向量$V=(V_1,V_2,...,V_L)^T$。
这个过程可以用~(\ref{equ:chap2:pooling})式表示。
\begin{equation}
  \label{equ:chap2:pooling}
  V_k=\frac{1}{F}\sum_{j=1}^{F}X_{(k-1)F+j}, k=1,2,...,L
\end{equation}

使用这样一种特征提取过程的优势在于，相对于直接使用信号频谱序列作为输入特征，
我们可以在保证其整体分布趋势近似不变的同时，大幅度地降低特征向量的维度，
从而提升分类器训练和预测过程的速度，保证诊断过程的是实时性。此外，在特征提取阶段
使用平均值下采样还可以降低某些特定频段的噪声所带来的影响，使得整个模型具有更好的
鲁棒性。此外，我们在使用平均值下采样的过程中，设定了一个参数$L$，也就是最终得到的
特征个数，我们会在之后的实验中对比该参数的变化对模型的影响，从而选择最佳的参数值。

图~\ref{fig:average_pooling}显示了一个采样点数为384的信号片段，其对应的频谱序列，
以及使用平均值下采样提取的特征个数为32的特征向量。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=8cm]{average_pooling}
  \caption{原始信号片段，RDFT频谱和特征向量}
  \label{fig:average_pooling}
\end{figure}

\subsection{分类模型}
\label{subsection:cascade_model}

虽然我们可以将提取到的特征向量直接输入到一个多分类模型中进行训练，就可以完成故障分类
的任务，但是在某些特定的情况下，尤其是当数据的类别标签具有明显的层次性（比如在故障诊断
问题当中，最常见的就是存在多种不同的故障位置，并且每一类故障位置可能会进一步有不同的
故障尺寸等等）的时候，设计一个由多个子分类器构成的级联分类器往往会更加有效。

假设在一个故障诊断问题中，存在三类可能的故障位置，我们记为位置A、位置B、位置C，
每一类故障位置都可能会发生3种不同程度的故障，我们记为尺寸1、尺寸2、尺寸3，同时还有一类
是正常样本。那么我们可以针对故障类别的这种层次性，设计一个由5个子分类器构成的级联分类
过程，如图~\ref{fig:cascade_classifier}所示。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=9cm]{cascade_classifier}
  \caption{级联分类过程示意图}
  \label{fig:cascade_classifier}
\end{figure}

图中每个菱形表示一个子分类器，可以根据实际情况采用不同的算法实现，不同子分类器所用的
算法也不一定相同。图中的每一个矩形框代表一种故障类型，可以看出这个例子中总共有10类样本，
其中一类是正常样本。可以看出，整个级联分类过程整体上分为三个阶段，第一个阶段由一个子分
类器判断是否为故障样本；接下来一个阶段由一个子分类器判断故障发生的位置；最后一个阶段
用三个子分类器分别判断故障的尺寸。

\section{实验验证}

\subsection{实验数据}

本文的所有实验所用的数据均是来自凯斯西储大学（Case Western Reserve University, CWRU）
的滚动轴承加速度测量数据，其中包括正常工作下的轴承数据和在各种不同故障位置和故障
尺寸下的轴承数据。在旋转机械故障诊断领域，该数据集已经成为标准的测试数据集之一，已经有很多
研究人员使用该数据集来验证其故障诊断模型。

数据采集的实验装置如图~\ref{fig:teststand}所示，测试台由一个2马力的三相感应电机（左）、
扭矩传感器和编码器（中间）、测力计（右）和控制电子设备（图中未显示）组成。 
用来收集振动数据的加速度传感器被磁性底座固定在轴承上方的机壳上，处于电机外壳的驱动端和
风扇端的12点钟位置。实验中使用16通道DAT记录仪收集振动信号，并在Matlab环境中进行后续处理。
所有数据文件均采用Matlab（*.mat）格式保存。该数据集以12kHz的采样频率收集和记录数据，另外
也采集了采样频率为48kHz时的数据，用于驱动端的轴承故障研究。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=6cm]{teststand}
  \caption{级联分类过程示意图}
  \label{fig:teststand}
\end{figure}

数据采集的实验过程中所用的轴承包括两种，一种是型号为6205-2RS JEM的SKF深沟型轴承，
另一种是型号为6203-2RS JEM的SKF深沟型轴承，表~\ref{tab:bearing_params}列出了型号为
6205-2RS JEM的SKF深沟型轴承的一些主要的几何参数。
\begin{table}[htb]
  \centering
  \begin{minipage}[t]{0.8\linewidth} % 如果想在表格中使用脚注，minipage是个不错的办法
  \caption{6205-2RS JEM SKF深沟型轴承几何参数}
  \label{tab:bearing_params}
    \begin{tabularx}{\linewidth}{lXXXXX}
      \toprule[1.5pt]
      滚珠个数 & 内圈直径（英寸）& 外圈直径（英寸）& 轴承厚度（英寸）& 滚珠直径（英寸）& 轴承节径（英寸） \\\midrule[1pt]
      9 & 0.9843 & 2.0472 & 0.5906 & 0.3126 & 1.537 \\\bottomrule[1.5pt]
    \end{tabularx}
  \end{minipage}
\end{table}

轴承的状态除了正常之外，还有内圈故障、钢球故障和外圈故障三种类型，使用火花放电
的方法在测试轴承上引入7密耳、14密耳、21密耳、28密耳（1密尔=0.001英尺）四种不同损伤直径的
单点故障。电机的负载可以通过风机来调节，可以产生0-3马力的载荷，从而可以研究不同负载情况下
轴承的故障特征。

为了验证本文在各章节中设计的故障诊断模型对故障位置及故障尺寸的识别能力，本文选取了在采样频率为
12kHz，负载为0马力情况下的12种不同类别的数据，对所设计的模型进行训练和测试。这12类样本对应
的故障位置和故障尺寸如表~\ref{tab:class_desc}所示。
\begin{table}[htb]
  \centering
  \begin{minipage}[t]{0.7\linewidth} % 如果想在表格中使用脚注，minipage是个不错的办法
  \caption{12类数据对应的故障位置和尺寸}
  \label{tab:class_desc}
    \begin{tabularx}{\linewidth}{XXX}
      \toprule[1.5pt]
      类别标签 & 故障位置 & 故障尺寸（英寸） \\\midrule[1pt]
      0 & 无 & 无 \\
      1 & 钢球 & 0.007 \\
      2 & 钢球 & 0.014 \\
      3 & 钢球 & 0.021 \\
      4 & 钢球 & 0.028 \\
      5 & 内圈 & 0.007 \\
      6 & 内圈 & 0.014 \\
      7 & 内圈 & 0.021 \\
      8 & 内圈 & 0.028 \\
      9 & 外圈 & 0.007 \\
      10 & 外圈 & 0.014 \\
      11 & 外圈 & 0.021 \\
      \bottomrule[1.5pt]
    \end{tabularx}
  \end{minipage}
\end{table}

\subsection{实验过程}

从表~\ref{tab:class_desc}中可以看到，除了一类正常数据，以及在故障尺寸为0.028
英尺时包含钢球和内圈两种故障位置，其余的三类故障尺寸都分别包含钢球、内圈和外
圈三种故障位置，一共是12类数据。根据~\ref{subsection:cascade_model}节中的级联
分类思路，以及本文所使用的数据的特点，我们设计了如图~\ref{fig:cascade_classifier_exp}
所示的级联故障诊断模型，分类器输出的类别标签与实际样本的对应关系参见表~\ref{tab:class_desc}。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=11cm]{cascade_classifier_exp}
  \caption{CWRU数据级联分类模型}
  \label{fig:cascade_classifier_exp}
\end{figure}

在这个级联故障诊断模型中，我们总共有5个子分类器，每个子分类器我们使用了支持向量机算法，
并且个分类器之间单独进行训练，最后在预测阶段按照图~\ref{fig:cascade_classifier_exp}中
显示的级联过程逐步进行预测，得到最终的预测标签。图中各分类器的功能描述如
表~\ref{tab:classifier_desc}所示，其中12类数据的标签对应的故障情况参见表~\ref{tab:bearing_params}。
\begin{table}[htb]
  \centering
  \begin{minipage}[t]{0.8\linewidth} % 如果想在表格中使用脚注，minipage是个不错的办法
  \caption{各子分类器详细说明}
  \label{tab:classifier_desc}
    \begin{tabularx}{\linewidth}{lXXX}
      \toprule[1.5pt]
      序号 & 输出标签个数 & 训练所用数据标签 & 分类器功能 \\\midrule[1pt]
      1 & 2 & 0-11 & 判断是否故障\\
      2 & 3 & 1-11 & 判断故障位置\\
      3 & 4 & 1-4 & 判断钢球故障尺寸\\
      4 & 4 & 5-8 & 判断内圈故障尺寸\\
      5 & 3 & 9-11 & 判断外圈故障尺寸\\
      \bottomrule[1.5pt]
    \end{tabularx}
  \end{minipage}
\end{table}

由于CWRU数据集中每一种类别的数据都存储在一个Matlab文件（*.mat）中，并且每一个文件中都
包含了一段至少12000个连续的采样点。因此，为了获得足够数量的样本对设计的故障诊断模型
进行训练和预测，我们从每个数据文件中随机地截取M个长度为N的信号片段，并且将其中的$3M/4$个
信号片段作为训练样本，其余$M/4$个信号片段作为预测样本。在选择$N$值的时候，一方面要保证能够
从有限的采样点中截取到足够多的信号片段来训练我们的模型，因此$N$值不能太大；另一方面要保证
截取的长度为N的信号片段中能够包含足够的信息，因此N值也不能太小。在本文的所有实验中
均使用$N=384$。

\subsection{实验结果及分析}
\label{subsection:cascade_result}

图~\ref{fig:series}是从表~\ref{tab:class_desc}中所列的12类数据中随机截取的信号片段的
原始波形，图中从左至右，从上到下依次对应类别标签为0-11的样本。每一类数据我们都随机地
显示了3信号样本。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=9cm]{series}
  \caption{各类信号片段原始波形}
  \label{fig:series}
\end{figure}

从图中可以看出，由于样本的截取过程是随即进行的，因此同一类样本间存在很大的相位差别，
因此如果直接使用原始信号片段来训练分类器的效果比较差。使用本章中基于下采样的特征提取方法，
对图~\ref{fig:series}中列出的12个信号频段进行特征提取，所获得的特征向量如图~\ref{fig:average_pooling_features_12}
所示。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=9cm]{average_pooling_features_12}
  \caption{基于RDFT和平均值下采样提取的特征向量}
  \label{fig:average_pooling_features_12}
\end{figure}

可以看出，与图~\ref{fig:series}中显示的原始信号片段相比，使用本章中的特征提取方法获得
的结果作为特征向量时，避免信号片段间相位的差异带来的分类难度，使得类别之间具有更好的区分性，
而同一类样本之间的相似程度也更高，因此能够提升最终的故障诊断精度。另一方面，图~\ref{fig:average_pooling_features_12}
中的特征向量的维度为12，相比于原始信号片段的维度384，极大程度地降低了特征的维度，加快了模型的训练过程。

此外，本章中设计的特征提取算法中的参数L，也就是最终得到的特征向量的维度，是可以人为指定的，
例如图~\ref{fig:average_pooling_features_12}中显示的是$L=12$时提取的特征向量，当$L=384$时，
相当于直接将信号片段的频谱用作特征向量，如~\ref{fig:average_pooling_features_384}所示。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=9cm]{average_pooling_features_384}
  \caption{维度为384的特征向量}
  \label{fig:average_pooling_features_384}
\end{figure}

选择适当的特征维度L也是影响模型精度的一个关键因素。如果L值过大，那么下采样在特征提取过程中
起到的作用就相对比较微弱，无法过滤掉信号种特定频段的一些噪声，同时过大的特征维度也会减慢
分类器的训练和预测速度；相反，如果L值过小，那么最终提取的特征向量将会丢失原始信号片段中的
过多信息，因此将会降低分类器的预测精度。因此，我们会从最终训练得到的分类器的预测精度和训练
过程所用时长两方面考虑，来最终选择合适的L值。

分别取特征个数L为3、6、12、24、48、96、192的情况下训练并测试级联分类模型中的5个子分类器，
子分类器使用的是支持向量机算法，核函数使用高斯核函数，算法的参数取值为惩罚因子$C=1$，
高斯核宽度参数$\sigma=L$。整个级联分类模型的预测精度和训练时间如表~\ref{tab:chap2_acc_time}
所示。
\begin{table}[htb]
  \centering
  \begin{minipage}[t]{0.6\linewidth} % 如果想在表格中使用脚注，minipage是个不错的办法
  \caption{不同特征个数下模型的预测精度和训练时间}
  \label{tab:chap2_acc_time}
    \begin{tabularx}{\linewidth}{lXX}
      \toprule[1.5pt]
      特征个数 & 模型预测精度 & 模型训练时间 \\\midrule[1pt]
      3   &  0.6932 & 2.23  \\
      6   &  0.7947 & 0.89  \\
      12  &  0.8784 & 1.03  \\
      24  &  0.9171 & 1.37  \\
      48  &  0.9381 & 2.01  \\
      96  &  0.9297 & 3.22  \\
      192 &  0.9349 & 5.89  \\
      384 &  0.9184 & 11.29 \\
      \bottomrule[1.5pt]
    \end{tabularx}
  \end{minipage}
\end{table}

模型的预测精度和模型训练所用时间随特征个数L的变化趋势如图~\ref{fig:chap2_accs}和
图~\ref{fig:chap2_times}所示。
\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=6cm]{chap2_accs}
  \caption{模型预测精度随特征个数的变化情况}
  \label{fig:chap2_accs}
\end{figure}

\begin{figure}[ht] % use float package if you want it here
  \centering
  \includegraphics[height=6cm]{chap2_times}
  \caption{模型训练时间随特征个数的变化情况}
  \label{fig:chap2_times}
\end{figure}

从图~\ref{fig:chap2_accs}和图~\ref{fig:chap2_times}可以看出，模型在特征个数取48时，
模型的预测精度达到最高，为93.81\%，并且训练所需时间也相对比较少。因此可以将$L=48$
作为最终的选择。此时模型在测试集上的混淆矩阵如表\ref{tab:chap2:confusion_matrix}所示。
\begin{table}[htb]
  \centering
  \begin{minipage}[t]{0.9\linewidth} % 如果想在表格中使用脚注，minipage是个不错的办法
  \caption{L=48时模型在测试集上的混淆矩阵}
  \label{tab:chap2:confusion_matrix}
    \begin{tabularx}{\linewidth}{XXXXXXXXXXXX}
      \toprule[1.5pt]
         0 &   1 &   2 &   3 &   4 &   5 &   6 &   7 &   8 &   9 &  10 &  11 \\\midrule[1pt]
      1579 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 \\
         0 & 697 &   2 &  11 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 \\
         0 &  83 & 682 &  18 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   1 \\
         0 &  20 &   0 & 765 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 \\
         0 &   0 &   0 &   0 & 777 &   0 &   0 &   0 &   0 &   0 &   0 &   0 \\
         0 &   0 &   0 &   0 &   0 & 780 &   0 &   0 &   0 &   0 &   0 &   0 \\
         0 &   0 &   0 &   0 &   0 &   0 & 784 &   0 &   0 &   0 &   0 &   0 \\
         0 &   0 &   0 &   0 &   0 &   0 &   0 & 786 &   0 &   0 &   0 &   0 \\
         0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 & 777 &   0 &   0 &   0 \\
         0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 & 785 &   0 &   0 \\
         0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 &   0 & 784 &   0 \\
         0 &   0 &   0 &   0 & 395 &   0 &   1 &   2 &   0 &  93 &   0 & 297 \\
      \bottomrule[1.5pt]
    \end{tabularx}
  \end{minipage}
\end{table}

从混淆矩阵可以看出，类别标签为0、5、8、10的测试样本的预测结果与样本的真实标签
完全一致，其他类别的样本或多或少都存在预测结果错误的情况，尤其是第11类样本（外圈
故障，尺寸为0.021）的样本，在784各样本中有491个样本预测错误。

\section{小结}

本章介绍了信号处理中经常用到的实数形式的傅立叶变换，以及传统的机器学习算法中经典的
支持向量机算法。之后详细介绍了本章中设计的基于单一尺度的故障特征提取方法和基于
支持向量机算法的级联诊断模型。特征提取阶段使用了平均值下采样来对信号片段频谱序列
的特征个数进行压缩，同时也能够减弱信号中特定频谱的噪声对诊断效果的影响。另外，由于我们
通过改变下采样的宽度来控制特征向量的维度，因此我们能够得到不同尺度下的特征，因此我们
通过对比实验考察了不同尺度的特征对诊断模型的影响，最后给出了能够使模型预测精度达到
最高的特征维度。但是我们也分析了该模型在测试集上的预测结果的混淆矩阵，发现在某些类别
的样本上表现并不理想，这也是后面的章节中设计的模型需要进一步提升的地方。
